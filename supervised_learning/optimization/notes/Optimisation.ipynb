{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c734f830",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "\n",
    "**Optimization is the process of finding the best solution to a problem**. In the context of Machine Learning, optimization is used to **find the best set of parameters for a model that minimizes a specific objective function**. The objective function is a measure of how well the model performs on a given task, such as classification or regression.\n",
    "\n",
    "Optimization is crucial in Machine Learning because it allows us to train models that can make accurate predictions on new data. Without optimization, it would be difficult to find the best set of parameters for a model, which would lead to poor performance and inaccurate predictions\n",
    "\n",
    "There are many different optimization algorithms that can be used in Machine Learning, such as gradient descent, stochastic gradient descent, and Adam. Each algorithm has its own strengths and weaknesses, and the choice of algorithm depends on the specific problem being solved and the characteristics of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85894268",
   "metadata": {},
   "source": [
    "## What is a hyperparameter\n",
    "\n",
    "A **hyperparameter is a parameter that is set before the training of a machine learning model begins**. These parameters **cannot be learned from the data during training, but instead, they must be set by the programmer** based on their domain knowledge, experience, and intuition.\n",
    "\n",
    "Hyperparameters *control the behavior of the training algorithm and the structure of the resulting model*. Some common hyperparameters (of a neural network) include the \n",
    "\n",
    "- learning rate\n",
    "\n",
    "- regularization strength\n",
    "\n",
    "- number of hidden layers\n",
    "\n",
    "- number of neurons per layer\n",
    "\n",
    "To find the best hyperparameters, a common approach is to perform a grid search or a randomized search over a range of values for each hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2c14f",
   "metadata": {},
   "source": [
    "## How and why do you normalize your input data?\n",
    "\n",
    "**Normalization is the process of scaling the input data to have zero mean and unit variance**. This is often done before training a machine learning model to improve performance and prevent numerical issues.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:744/1*HW7-kYjj6RKwrO-5WTLkDA.png\" alt=\"logo\" width=\"600\"/>\n",
    "\n",
    "Normalization can improve performance by ensuring that all features are treated equally during training. This is because features with larger magnitudes can dominate the training process and overshadow other features. Normalization can also prevent numerical issues, such as overflow or underflow, that can occur when working with very large or very small numbers.\n",
    "\n",
    "There are several ways to normalize features\n",
    "\n",
    "#### 1. Standartization\n",
    "\n",
    "Standardisation replaces the values by their `Z` scores. This redistributes the features with their mean $\\mu = 0$ and standard deviation $\\sigma = 1$.\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - x_{mean}}{\\sigma}\n",
    "$$\n",
    "\n",
    "#### 2. Mean Normalisation\n",
    "\n",
    "This distribution will have values between `-1` and `1` with $\\mu = 0$.\n",
    "\n",
    "Standardisation and Mean Normalization can be used for algorithms that assumes zero centric data like Principal Component Analysis(PCA).\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - mean(x)}{max(x) - min(x)}\n",
    "$$\n",
    "\n",
    "### 3. Min Max Scaling\n",
    "\n",
    "This scaling brings the value between `0` and `1`.\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - min(x)}{max(x) - min(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed44610",
   "metadata": {},
   "source": [
    "## What is a saddle point?\n",
    "\n",
    "A saddle point is a critical point in the optimization landscape of a function where the gradient is zero, i.e., the **function has a stationary point** in one direction and a maximum or minimum point in another direction.\n",
    "\n",
    "At a saddle point, **the gradient descent algorithm will get stuck and cannot make progress towards the minimum**. This is because the gradient is zero, and the direction of steepest descent does not point towards the minimum. Saddle points are common in high-dimensional optimization problems, especially in deep neural networks.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/367454403/figure/fig1/AS:11431281115169415@1674789822813/a-Schematic-of-local-minima-and-saddle-point-in-a-onedimension-function-b-Schematic.ppm\" alt=\"logo\" width=\"700\"/>\n",
    "\n",
    "\n",
    "To deal with saddle points, several techniques have been proposed, such as momentum-based optimization, adaptive learning rates, and second-order optimization methods. These techniques can help the optimization algorithm escape saddle points and converge to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284bbd5",
   "metadata": {},
   "source": [
    "## What is stochastic gradient descent?\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a widely used optimization algorithm in machine learning. It is a faster, more scalable variant of traditional (batch) Gradient Descent.\n",
    "\n",
    "In traditional Gradient Descent, the algorithm computes the gradient of the loss function using the entire training dataset before updating the model parameters. While this ensures an accurate gradient, it becomes computationally expensive and slow when the dataset is large.\n",
    "\n",
    "SGD, in contrast, updates the model parameters using only a small, randomly selected subset of the data (a mini-batch or even a single example). This makes each update much faster and allows the model to start learning immediately. However, because it's based on a subset, the updates are noisier and less stable — but this noise can help the algorithm escape local minima and potentially find better solutions.set is large. After computing the gradient on the mini-batch, the algorithm updates the model parameters in the direction of the negative gradient.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1005/1*_6TVU8yGpXNYDkkpOfnJ6Q.png\" alt=\"logo\" width=\"700\"/>\n",
    "\n",
    "\n",
    "To further improve the performance of SGD, several variants have been proposed, such as momentum-based SGD, Adagrad, RMSProp, and Adam. These variants adjust the learning rate or the update direction of the algorithm based on the history of the gradients, which can improve convergence and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea469d5f",
   "metadata": {},
   "source": [
    "## What is mini-batch gradient descent?\n",
    "\n",
    "Mini-Batch Gradient Descent is a popular optimization algorithm that blends the advantages of Batch Gradient Descent and Stochastic Gradient Descent (SGD).\n",
    "\n",
    "- **Batch Gradient Descent** uses the entire dataset to compute the gradient for every update. This is stable but can be slow and memory-intensive for large datasets.\n",
    "- **SGD** updates the model using just one training example at a time, making it faster and more memory-efficient, but also noisy and less stable.\n",
    "\n",
    "Mini-Batch Gradient Descent strikes a balance: it updates the model using a small random subset (mini-batch) of the training data. This gives better convergence stability than SGD and is computationally more efficient than full batch gradient descent.\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/zi2yef4nw297/68onQ2rMp0xMJ21MUiqdxG/245b041955c951da3f8c887db4f307de/batch__stochastic__mini-batch_gradient_descent-Dec-22-2022-04-32-42-4986-AM.png?fm=webp&w=1140&q=99\" alt=\"logo\" width=\"800\"/>\n",
    "\n",
    "\n",
    "### Why use Mini-Batches?\n",
    "- More efficient than full batch updates.\n",
    "- More stable than single-example (SGD) updates.\n",
    "- Leverages parallel processing on modern hardware (e.g., GPUs).\n",
    "- Introduces controlled noise to help escape local minima.\n",
    "\n",
    "**The size of the mini-batch is a hyperparameter that needs to be chosen carefully**. A larger mini-batch can provide a more accurate estimate of the gradient but may slow down the training process and require more memory. A smaller mini-batch can be faster and use less memory but may result in noisy updates to the model parameters\n",
    "\n",
    "### Choosing the Mini-Batch Size\n",
    "The mini-batch size is a tunable hyperparameter:\n",
    "\n",
    "- Large batch: more accurate gradients, slower updates, higher memory use.\n",
    "- Small batch: faster updates, lower memory, noisier convergence.\n",
    "\n",
    "Typical mini-batch sizes: 32, 64, 128, but this depends on the model and hardware.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0756eed1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## What is the moving average\n",
    "\n",
    "In neural networks, a moving average is a technique used to smooth parameter updates during training. It helps reduce noise and fluctuations in model parameters like weights or batch normalization statistics, making training more stable and improving generalization.\n",
    "\n",
    "The basic idea behind the moving average is to **keep track of the past values of the parameters and update them based on a weighted average of the current and past values**. The weights are typically chosen to give more weight to the current value and less weight to the past values, with the goal of adapting to the changes in the training data while maintaining stability.\n",
    "\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTksQa7F7MyYdQiY3LEUqHQjr4gki88vmVnig&s\" alt=\"logo\" width=\"400\"/>\n",
    "\n",
    "\n",
    "### When and Why It's Useful\n",
    "- Stability: Helps reduce noisy updates, especially in high-variance optimizers like SGD.\n",
    "- Generalization: Averaged parameters tend to generalize better on unseen data.\n",
    "- Prediction: Often, the moving-averaged parameters are used for evaluation/inference, not the raw ones.\n",
    "\n",
    "\n",
    "#### Math behind it\n",
    "\n",
    "\n",
    "The moving average maintains a running estimate of a parameter by combining its current value with its previous average. A common form is the Exponential Moving Average (EMA):\n",
    "\n",
    "The **moving average calculates the weighted moving average of a dataset**. \n",
    "\n",
    "\n",
    "It iterates through the data points, updating the weighted average using the current data point and a weighting factor (`beta`). \n",
    "\n",
    "The formula used for the calculation is\n",
    "$$\n",
    "MA = (\\beta \\cdot w + (1 - \\beta) \\cdot d)\n",
    "$$\n",
    "\n",
    "- `w` represents the previous weighted average\n",
    "- `d` is the current data point\n",
    "- $\\beta$ is the weighting factor\n",
    "\n",
    "Additionally, the function applies bias correction to adjust the weighted average, ensuring accuracy over time. Formula for bias correction is\n",
    "\n",
    "$$\n",
    "w_{new} = \\frac{w}{1 - \\beta^{i+1}}\n",
    "$$\n",
    "\n",
    "- $w_{new}$ is the corrected weighted average\n",
    "- $w$ is the current weighted average\n",
    "- $\\beta$ is the weight parameter\n",
    "- `i` is the index of the current data point\n",
    "\n",
    "The resulting list contains the moving averages of the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d86700f",
   "metadata": {},
   "source": [
    "## What is gradient descent with momentum? \n",
    "\n",
    "Gradient Descent with Momentum is an optimization technique designed to accelerate convergence and stabilize updates in training machine learning models, especially deep neural networks.\n",
    "\n",
    "In standard gradient descent, parameter updates can be slow and noisy, especially in regions where the cost surface is uneven or where gradients oscillate (e.g., ravines or saddle points).\n",
    "\n",
    "Momentum adds memory to the optimization process by incorporating a moving average of past gradients into the update. This helps:\n",
    "\n",
    "- Smooth out oscillations\n",
    "- Speed up convergence\n",
    "- Avoid getting stuck in shallow local minima\n",
    "\n",
    "The formula for updating the variables with momentum is:\n",
    "\n",
    "$$\n",
    "dW = \\beta_1 \\cdot v + (1 - \\beta_1) \\cdot grad\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $dW$ is the update term\n",
    "- $\\beta_1$ is the momentum weight\n",
    "- $v$ is the previous first moment of the variable\n",
    "- $grad$ is the gradient of the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876e715",
   "metadata": {},
   "source": [
    "## What is RMSProp?\n",
    "\n",
    "RMSProp stands for Root Mean Square Propagation, and it is an **optimization algorithm** commonly used in deep learning. RMSProp is designed to **handle the problem of diminishing learning rates**, which can lead to slow convergence or even a complete halt in the training process.\n",
    "\n",
    "#### Intuition in Simple Words\n",
    "\n",
    "- RMSProp changes how quickly the model learns for each part of the problem. If some parts are changing a lot, it slows down the learning to avoid overreacting. If other parts change slowly, it speeds up learning to not fall behind.\n",
    "- It does this by keeping track of how much each part has changed recently. If something changes a lot, it gets less attention. If something changes slowly, it gets more attention.\n",
    "- This helps the model learn steadily without getting too distracted by sudden changes or missing out on slower changes.\n",
    "\n",
    "#### Key Idea\n",
    "\n",
    "The basic idea behind RMSProp is to **maintain a moving average of the squared gradient for each weight in the network. This average is then used to scale the learning rate for each weight in the network**. In other words, the algorithm adapts the learning rate for each weight based on the magnitude of its gradients.\n",
    "\n",
    "RMSProp can be implemented in various deep learning frameworks such as TensorFlow, PyTorch, or Keras by using their built-in optimization algorithms that support RMSProp.\n",
    "\n",
    "\n",
    "Here are the key formulas\n",
    "\n",
    "##### Squared Gradient Calculation\n",
    "\n",
    "This formula calculates the squared gradient, which is a weighted sum of the squared gradients of previous time steps and the current gradient\n",
    "$$\n",
    "sq_{gradient} = \\beta_2 \\cdot s + (1 - \\beta_2) \\cdot grad^2\n",
    "$$\n",
    "\n",
    "- $sq_{gradient}$ represents the squared gradient, which is a weighted sum of the squared gradients of previous time steps and the current gradient\n",
    "- $\\beta_2$ is the RMSProp weight, controlling the contribution of the previous squared gradients\n",
    "- $s$ denotes the previous second moment of the variable\n",
    "- $grad$ is the gradient of the variable\n",
    "\n",
    "##### Variable update\n",
    "\n",
    "This formula updates the variable using the RMSProp update rule. It adjusts the variable based on the learning rate $\\alpha$, the gradient, the square root of the squared gradient, and a small constant \n",
    "$\\epsilon$ to avoid division by zero.\n",
    "\n",
    "$$\n",
    "updated_{variable} = var - \\frac{\\alpha \\cdot grad}{\\sqrt{sq_{gradient}} + \\epsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ab23f",
   "metadata": {},
   "source": [
    "## What is Adam optimization?\n",
    "\n",
    "Adam optimization is an algorithm used for stochastic gradient descent that is widely used in deep learning for optimizing neural networks. Adam stands for Adaptive Moment Estimation and it combines the best aspects of two other popular optimization algorithms, namely RMSProp and momentum.\n",
    "\n",
    "\n",
    "#### Intuition in Simple Words\n",
    "\n",
    "- Adam combines the benefits of two other optimization techniques: RMSProp and Momentum.\n",
    "- Like RMSProp, it adjusts learning rates for different parameters, but it also keeps track of past gradients' momentum.\n",
    "- This combination allows Adam to adaptively adjust the learning rate for each parameter while efficiently handling sparse gradients and noisy data.\n",
    "\n",
    "\n",
    "Like RMSProp, Adam computes the moving average of the squared gradients to adapt the learning rate for each parameter. And like momentum, Adam keeps track of the exponentially decaying average of past gradients to smooth out the parameter updates.\n",
    "\n",
    "#### Key elements\n",
    "\n",
    "The algorithm is called Adam. It is not an acronym and is not written as ADAM. \n",
    "- The name Adam is derived from adaptive moment estimation.\n",
    "- Straightforward to implement.\n",
    "- Computationally efficient.\n",
    "- Little memory requirements.\n",
    "\n",
    "*The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients*\n",
    "\n",
    "\n",
    "The authors describe Adam as combining the advantages of two other extensions of stochastic gradient descent. Specifically:\n",
    "\n",
    "- **Adaptive Gradient Algorithm (AdaGrad)** that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
    "- **Root Mean Square Propagation (RMSProp)** that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
    "\n",
    "Adam realizes the benefits of both AdaGrad and RMSProp.\n",
    "\n",
    "Instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance).\n",
    "\n",
    "#### Mathematical reasoning and formulas\n",
    "\n",
    "##### Initialization\n",
    "\n",
    "Adam optimizer initializes two variables, `v` and `s`, which represent the exponentially decaying average of past gradients and the exponentially decaying average of past squared gradients, respectively.\n",
    "\n",
    "##### Update rule\n",
    "\n",
    "At each iteration, Adam computes the exponentially decaying average of past gradients  `v` using a decay factor $\\beta_1$ and the exponentially decaying average of past squared gradients \n",
    "`s` using decay factor $\\beta_2$. These averages are updated with the current gradient of the variable.\n",
    "\n",
    "$$\n",
    "v_t = \\beta_1 \\cdot v_{t-1} + (1 - \\beta_1) \\cdot grad\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "s_t = \\beta_2 \\cdot s_{t-1} + (1 - \\beta_2) \\cdot grad^2\n",
    "$$\n",
    "\n",
    "##### Bias correction\n",
    "\n",
    "To compensate for the initialization bias towards zero, Adam performs bias correction by scaling the computed averages `v` and `s` by factors\n",
    "\n",
    "$$\n",
    "\\frac{1}{1 - \\beta_1^t}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{1}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "where `t` is the current time step.\n",
    "\n",
    "##### Variable update\n",
    "\n",
    "Variable Update: Finally, Adam updates the variable using the corrected averages `v` and `s`, along with a small constant  $\\epsilon$ to prevent division by zero. The variable is updated in the direction of the gradient, with a learning rate $\\alpha$ scaled by the square root of the corrected squared gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d1121-1004-4c8c-a7f9-73b96e5b1c7a",
   "metadata": {},
   "source": [
    "### Summary of Optimization Algorithms in Deep Learning\n",
    "\n",
    "| **Method** | **What it is** | **Key Idea** | **Update Rule / Implementation Summary** | **Pros** | **Cons** |\n",
    "|------------|----------------|--------------|------------------------------------------|----------|----------|\n",
    "| **Stochastic Gradient Descent (SGD)** | A variant of gradient descent that uses **one training example** at a time to compute gradients | Fast updates using a single example | θ = θ - α × ∇J(θ; xᵢ, yᵢ) | Fast, memory-efficient | Noisy updates, can fluctuate |\n",
    "| **Mini-Batch Gradient Descent** | Middle-ground between batch and SGD, using **small subsets (mini-batches)** | Smoother updates than SGD, faster than batch | θ = θ - α × ∇J(θ; mini-batch) | Efficient, stable updates | Requires tuning batch size |\n",
    "| **Moving Average (e.g., EMA)** | A technique to **smooth parameter values or metrics** over time | Track a running average of weights or values | θ_avg = β × θ_avg + (1 - β) × θ | Reduces noise, improves generalization | Not an optimizer by itself |\n",
    "| **Momentum** | Enhances SGD by adding **velocity** based on past gradients | Accelerates in consistent directions, dampens oscillations | v = β × v + (1 - β) × ∇J(θ); θ = θ - α × v | Faster convergence, smoother updates | Still sensitive to learning rate |\n",
    "| **RMSProp** | Adaptive learning rate algorithm that **scales learning by recent gradient magnitudes** | Divides gradient by moving average of past squared gradients | E[g²] = β × E[g²] + (1 - β) × g²; θ = θ - α × g / √(E[g²] + ε) | Works well in non-stationary problems | Sensitive to hyperparams |\n",
    "| **Adam (Adaptive Moment Estimation)** | Combines **Momentum + RMSProp** | Tracks **both** moving average of gradient and squared gradient | m = β₁ × m + (1 - β₁) × g; v = β₂ × v + (1 - β₂) × g²; θ = θ - α × m̂ / (√v̂ + ε) | Best of both worlds, widely used | Slightly more computational cost |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Implementation Essentials\n",
    "\n",
    "| **Method** | **Hyperparameters** | **Common Values** | **Used In** |\n",
    "|------------|---------------------|-------------------|-------------|\n",
    "| **SGD** | Learning rate (α) | 0.01, 0.001 | All tasks (baseline) |\n",
    "| **Mini-Batch GD** | α, batch size | 32, 64, 128 | Image, text, large datasets |\n",
    "| **Moving Average** | Decay (β) | 0.99, 0.999 | EMA of weights, metrics |\n",
    "| **Momentum** | α, β (momentum) | 0.9 | Deep NNs, noisy gradients |\n",
    "| **RMSProp** | α, β, ε | α = 0.001, β = 0.9, ε = 1e-8 | RNNs, online learning |\n",
    "| **Adam** | α, β₁, β₂, ε | α = 0.001, β₁ = 0.9, β₂ = 0.999 | Most deep learning tasks |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b1bea",
   "metadata": {},
   "source": [
    "## What is learning rate decay?\n",
    "\n",
    "Learning rate decay is a technique used in optimization algorithms to decrease the learning rate during training. The idea behind learning rate decay is to gradually reduce the learning rate as training progresses, so that the updates become smaller and finer as the optimizer gets closer to the optimal solution. \n",
    "\n",
    "The main motivation behind learning rate decay is to balance the trade-off between convergence speed and convergence accuracy. A high learning rate can lead to fast convergence, but it can also result in overshooting the optimal solution and bouncing around it. A low learning rate can lead to a more accurate convergence, but it can also result in very slow convergence and getting stuck in local optima.\n",
    "\n",
    "\n",
    "#### Learning rate decay \n",
    "\n",
    "is a technique used in training machine learning models to gradually reduce the learning rate over time. This can help fine-tune the model's parameters more precisely as training progresses. Here's a summary of the learning rate decay method implemented in the provided code\n",
    "\n",
    "#### Factor Calculation: \n",
    "\n",
    "The decay rate is applied to the learning rate based on the global step divided by the decay step. This factor increases over time as the global step increases.\n",
    "\n",
    "$$\n",
    "factor = 1 + decay \\cdot \\frac{global_{step}}{decay_{step}}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Learning Rate Scaling\n",
    "\n",
    "The original learning rate ($\\alpha$) is scaled by the inverse of the calculated factor. This scaled learning rate is then used for training the model.\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{\\alpha}{factor}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1535fde",
   "metadata": {},
   "source": [
    "### Happy coding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
