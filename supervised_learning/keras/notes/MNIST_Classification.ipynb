{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z6shD1SL8w_"
   },
   "source": [
    "<img src=\"../holberton_logo.png\" alt=\"logo\" width=\"500\"/>\n",
    "\n",
    "# Classification of Hand Written Digits\n",
    "This example shows how to build a feed forward neural network that can be used to recognize images of hand-written digits, from 0-9.\n",
    "\n",
    "Our workflow will consist of the following steps\n",
    "- import libraries needed to compile and train the neural network and visualize data\n",
    "- get the MNIST dataset of handwritten digits\n",
    "- preprocess the data\n",
    "- split the dataset into data used for training and testing\n",
    "- build the neural network\n",
    "- train the neural network\n",
    "- test the neural network by showcasing the classification for some random instances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upbDs27BM_oF"
   },
   "source": [
    "## Import libraries\n",
    "- tensorflow\n",
    "- keras\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RQXspm48M0Q0"
   },
   "outputs": [],
   "source": [
    "# Let's import the libraries we'll need\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rtIhF0ZNJ2U"
   },
   "source": [
    "## 1. Load the MNIST dataset\n",
    "\n",
    "The MNIST dataset consists of `70,000` grayscale images of handwritten digits (`0-9`) with a resolution of `28x28` pixels. It is split into:\r\n",
    "\r\n",
    "- **Training set*``* 600``00 images (`x_train`) with corresponding labels (`y_train`).\r\n",
    "- **Test st:``** 0,``000 images (`x_test`) with corresponding labels (`y_test`).\r\n",
    "\r\n",
    "Each image is represented as a **28x28 NumPy array**, and the labels are integers from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVSNpDLJNP7O",
    "outputId": "aa4c866d-52f0-4d4a-fcb7-48eb8ca8bf6d"
   },
   "outputs": [],
   "source": [
    "# Now, let's load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset Structure\n",
    "\n",
    "Let's check the shape of the dataset and visualize some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Training labels shape: (60000,)\n",
      "Test data shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Print dataset shapes\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding MNIST Data Format\n",
    "\n",
    "Each image in the dataset is stored as a **28x28 NumPy array** with pixel values ranging from 0 to 255.  \n",
    "The labels are integers (0-9) representing the corresponding digit.\n",
    "\n",
    "Let's examine one training image and its label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Image Array:\n",
      " [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "\n",
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "# Print a single image array and its label\n",
    "print(\"Sample Image Array:\\n\", x_train[0]) \n",
    "print(\"\\nLabel:\", y_train[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the dataset by displaying sample images along with their labels, helping to understand the data distribution and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display some images with labels\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpChqoPUNapG"
   },
   "source": [
    "## 2. Preprocess the data\n",
    "\n",
    "### 2.1 Data Normalization\r\n",
    "\r\n",
    "Pixel values in the MNIST dataset range from **0 to 255** (grayscale intensity).  \r\n",
    "To improve model performance and training stability, we scale the values to a **0-1 range** by dividing by 255.  \r\n",
    "This helps neural networks converge faster and perform bettCode:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "24xRNwvlNj7p"
   },
   "outputs": [],
   "source": [
    "# Next, we'll preprocess the data by scaling it to a range of 0-1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nzxb4piNmCS"
   },
   "source": [
    "### 2.2 Split the dataset in train, validation and test dataset\n",
    "\r\n",
    "To fine-tune our model, we split the dataset into three parts:  \r\n",
    "\r\n",
    "- **Training set**: Used to train the model (remaining 55,000 images).  \r\n",
    "- **Validation set**: A small subset (5,000 images) used to evaluate performance during training and adjust hyperparameters.  \r\n",
    "- **Test set**: Separate (10,000 images) used for final model evaluation.\r\n",
    "\r\n",
    "We extract the first **5,000 images** from the training set for validation and keep the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PeX1m04qNxlv"
   },
   "outputs": [],
   "source": [
    "# now, we will split the dataset in train, validation and test dataset\n",
    "x_val = x_train[:5000] # validation dataset\n",
    "x_train = x_train[5000:] # train dataset\n",
    "y_val = y_train[:5000] # validation labels\n",
    "y_train = y_train[5000:] # train labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0MFxOOlN1DG"
   },
   "source": [
    "## 3. Build Neural Network \n",
    "\n",
    "\r\n",
    "We use a **Sequential** model, which stacks layers in order. The architecture consists of:\r\n",
    "\r\n",
    "1. **Flatten Layer**: Converts the **28×28** image into a **1D array of 784 values**, making it suitable for a fully connected network.  \r\n",
    "2. **Dense Layer (Hidden Layer)**: A fully connected layer with **128 neurons** and **ReLU activation**, which helps learn complex patterns.  \r\n",
    "3. **Output Layer**: A **Dense layer with 10 neurons** (one for each digit 0-9) and **softmax activation**, which outputs class proities of babilhandwritten digCode:\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Neural Network Architecture\n",
    "\n",
    "Our model follows a **feedforward neural network** structure with three key layers. Each plays a crucial role in transforming the input data into meaningful predictions.\n",
    "\n",
    "### 1. **Flatten Layer (`Flatten(input_shape=(28, 28))`)**  \n",
    "   - **Role**: Converts the **2D (28×28)** image into a **1D (784 values)** array.  \n",
    "   - **Intuition**: Fully connected (Dense) layers require a 1D input, so we reshape the pixel grid into a single vector while preserving the pixel values.\n",
    "\n",
    "### 2. **Hidden Dense Layer (`Dense(128, activation='relu')`)**  \n",
    "   - **Role**: A fully connected layer with **128 neurons** that learns hierarchical features.  \n",
    "   - **Intuition**:  \n",
    "     - Each neuron processes different aspects of the input.  \n",
    "     - The **ReLU activation function (`max(0, x)`)** introduces non-linearity, helping the model learn complex patterns.  \n",
    "     - The number **128** is a hyperparameter that can be adjusted for performance.\n",
    "\n",
    "### 3. **Output Layer (`Dense(10, activation='softmax')`)**  \n",
    "   - **Role**: Produces the final classification output with **10 neurons** (one per digit).  \n",
    "   - **Intuition**:  \n",
    "     - Each neuron corresponds to a digit (0-9).  \n",
    "     - The **softmax activation function** converts raw scores into **probabilities**, ensuring that the outputs sum to 1.  \n",
    "     - The model predicts the class with the highest probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "B3UDlbDMOHX0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evisp\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Now, let's define our neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # flatten the image to 1D array\n",
    "    keras.layers.Dense(128, activation='relu'),  # first dense layer with 128 neurons and relu activation\n",
    "    keras.layers.Dense(10, activation='softmax') # output layer with 10 neurons (one for each class) and softmax activation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m100,480\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsWa5wpEOJJb"
   },
   "source": [
    "## 4. Compile the model\n",
    "\n",
    "\r\n",
    "Before training, we need to **compile** the model by specifying:\r\n",
    "\r\n",
    "1. **Optimizer (`adam`)**  \r\n",
    "   - **Role**: Adjusts model weights to minimize errors during training.  \r\n",
    "   - **Why Adam?** Combines momentum and adaptive learning rates for **efficient** and **robust** optimization.  \r\n",
    "\r\n",
    "2. **Loss Function (`sparse_categorical_crossentropy`)**  \r\n",
    "   - **Role**: Measures the difference between predicted probabilities and actual labels.  \r\n",
    "   - **Why Sparse?** Suitable for integer labels (0-9) instead of one-hot encoding.  \r\n",
    "\r\n",
    "3. **Evaluation Metric (`accuracy`)**  \r\n",
    "   - **Role**: Tracks how often predictions match the tpilation Code:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GQ2xSoaSOUk1"
   },
   "outputs": [],
   "source": [
    "# Now, let's compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRrolM9iOWgo"
   },
   "source": [
    "## 5. Train the model\n",
    "\n",
    "\r\n",
    "To train the model, we use the **`fit`** method, which performs the following:\r\n",
    "\r\n",
    "1. **Training Data (`x_train`, `y_train`)**  \r\n",
    "   - The input images and corresponding labels used for model training.\r\n",
    "\r\n",
    "2. **Epochs (`epochs=5`)**  \r\n",
    "   - The number of times the entire training dataset is passed through the model.  \r\n",
    "   - In this case, we train the model for **5 epochs**, allowing the model to learn and improve iteratively.\r\n",
    "\r\n",
    "3. **Validation Data (`validation_data=(x_val, y_val)`)**  \r\n",
    "   - A separate dataset used to evaluate the model after each epoch, helping monitor **overfitting**.\r\n",
    "\r\n",
    "4. **History**  \r\n",
    "   - The **`history`** object stores training and validation metrics (like loss and accuracy) for later analysis or # Training Code:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBvPQHZwOfKd",
    "outputId": "07965b0c-e407-437e-f5e4-e5bfcdc5a419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8758 - loss: 0.4369 - val_accuracy: 0.9620 - val_loss: 0.1385\n",
      "Epoch 2/5\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9636 - loss: 0.1262 - val_accuracy: 0.9660 - val_loss: 0.1097\n",
      "Epoch 3/5\n",
      "\u001b[1m 423/1719\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9783 - loss: 0.0799"
     ]
    }
   ],
   "source": [
    "# Now, let's train the model, and also save the history of training and validation\n",
    "history = model.fit(x_train, y_train, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXujCeLfOp85"
   },
   "source": [
    "## 6. Evaluating the Model\r\n",
    "\r\n",
    "After training, we evaluate the model on the **test set** to determine how well it generalizes to unseen data.\r\n",
    "\r\n",
    "1. **Test Data (`x_test`, `y_test`)**  \r\n",
    "   - The **test set** is used to assess the model's performance after training, ensuring the model isn't overfitting to the training data.\r\n",
    "\r\n",
    "2. **Test Loss and Accuracy**  \r\n",
    "   - **Loss**: Measures the error between predicted and true labels on the test data.  \r\n",
    "   - **Accuracy**: Indicates the proportion of correct predictions made by ttion Code:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyPS4ausOzBF",
    "outputId": "64a0363b-beca-491c-ce7c-d00787ab41e3"
   },
   "outputs": [],
   "source": [
    "# Now, let's evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJo9gNOmO2Tq"
   },
   "source": [
    "## Visualize data\n",
    "- use `matplotlib.pyplot` to plot the training and validation accuracy and loss\n",
    "- display the number of epochs and the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "A3vtLQWoPC-i",
    "outputId": "45a625bc-67da-4e18-bb48-26969a4dc849"
   },
   "outputs": [],
   "source": [
    "# Now, let's plot the training and validation accuracy and loss\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "id": "1Zvph2fLte6h",
    "outputId": "2e36f49d-6ec5-40d9-f2df-c3b54d510778"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A **confusion matrix** helps to visualize the performance of the model by showing the actual versus predicted classifications. It provides insights into which digits the model struggles with.\n",
    "\n",
    "- **Diagonal elements** represent correct predictions.\n",
    "- **Off-diagonal elements** show misclassifications.\n",
    "\n",
    "We will generate the confusion matrix using `sklearn.metrics.confusion_matrix`, which compares the predicted labels with the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbEOCZu9PF9Y"
   },
   "source": [
    "## Visualize the classification of several images\n",
    "- select 20 images at random from the entire dataset\n",
    "- visualize the images alongside their labels in a grid-like plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "4dnGpQviu7gS",
    "outputId": "c6b7a11a-f1f1-4c79-c60f-c83943fffaba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "random_indices = np.random.randint(0, len(x_train), 20)\n",
    "random_images = x_train[random_indices]\n",
    "random_labels = y_train[random_indices]\n",
    "\n",
    "fix, axes = plt.subplots(4, 5, figsize=(10, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "  ax.imshow(random_images[i], cmap='gray')\n",
    "  ax.axis(\"off\")\n",
    "  ax.set_title(str(random_labels[i]))\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk4oazStPWq9"
   },
   "source": [
    "## Congrats on building your (first) neural network"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
