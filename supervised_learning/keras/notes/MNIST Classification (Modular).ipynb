{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcd55fc-95ba-4b97-b43a-c51649f71ebb",
   "metadata": {},
   "source": [
    "## MNIST Classification Project\n",
    "\n",
    "### Goal\n",
    "The objective of this project is to build, train, evaluate, and persist a neural network model for handwritten digit classification using the MNIST dataset. This includes:\n",
    "- Designing a customizable Keras model with regularization\n",
    "- Training the model with early stopping and best model checkpointing\n",
    "- Evaluating performance on the test set\n",
    "- Saving and loading the model\n",
    "\n",
    "\n",
    "### Dataset: MNIST\n",
    "MNIST (Modified National Institute of Standards and Technology) is a classic dataset of handwritten digits (0–9). It is commonly used for benchmarking classification algorithms.\n",
    "\n",
    "#### Size:\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "- Each image: 28 × 28 pixels\n",
    "- Grayscale (1 channel)\n",
    "- Flattened to a 784-dimensional vector for fully connected networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb80d689-0010-4a98-83a1-e55d80956de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6063d6eb-d7a3-4b9a-bbfa-3f1a1678fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afde6fe-5e58-4157-bc18-00203a688ae1",
   "metadata": {},
   "source": [
    "### Checking Dataset Shapes\n",
    "Before training, it's important to understand the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3704c06-3414-49e6-9ebf-c7659d4009e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "y_train shape: (60000,)\n",
      "x_test shape: (10000, 28, 28)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81409b-ffff-4c49-81b0-8b5e1bb64787",
   "metadata": {},
   "source": [
    "### Normalizing the Dataset\n",
    "Normalization is an essential preprocessing step that scales pixel values between 0 and 1. This helps in faster and more stable training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44023165-ba7a-482a-806e-e94324d3499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train.astype('float32'), axis=1)\n",
    "x_test = normalize(x_test.astype('float32'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053341e4-2a8c-4d88-90cb-30294515bafc",
   "metadata": {},
   "source": [
    "### Reshaping the Dataset\n",
    "Before feeding the data into a neural network, it's essential to flatten the 2D images (28x28 pixels) into 1D vectors (784 values per image). This step ensures compatibility with fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0d817d5-84b9-4002-ab39-cd83dcd8094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e552b-fafa-40e5-a141-f3c13bf2a5f1",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "One-hot encoding is a technique used to convert categorical labels into a binary matrix. In the context of classification problems like MNIST, each label (which represents a digit between 0 and 9) is converted into a vector where only one element is \"1\", and all other elements are \"0\".\n",
    "\n",
    "#### Why is it needed?\n",
    "In machine learning, especially for classification tasks with neural networks, the output labels must be in a format that can be used for training the model. Neural networks typically use softmax activation in the output layer for multi-class classification, which produces a probability distribution across the classes (i.e., a vector of probabilities that sum to 1).\n",
    "\n",
    "For this to work correctly, we need to represent the labels in a format that matches the output. One-hot encoding is a natural fit for this because it ensures each label corresponds to a specific class (position) in the output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5596ff4b-0e79-4162-a546-fd3c55b11020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def one_hot(labels, classes=None):\n",
    "    \"\"\"\n",
    "        function that converts a label vector into a one-hot matrix\n",
    "\n",
    "        :param labels: labels\n",
    "        :param classes: nbr of classes\n",
    "\n",
    "        :return: one-hot matrix, shape(labels,classes)\n",
    "    \"\"\"\n",
    "    return K.utils.to_categorical(labels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dee5ea1-41f1-4b19-a34f-6ba374c0811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = one_hot(y_train, 10)\n",
    "y_test_one_hot = one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04feee-dcd6-4424-be6d-6d257341a936",
   "metadata": {},
   "source": [
    "### Model Overview & Architecture\r\n",
    "\r\n",
    "This model is a fully connected neural network designed for classification tasks, like the MNIST digit classification problem.\r\n",
    "\r\n",
    "#### **High-Level Architecture**:\r\n",
    "\r\n",
    "1. **Input Layer**: \r\n",
    "   - The model accepts input data with `nx` features. For MNIST, each input image is flattened into a 784-dimensional vector (28x28 pixels).\r\n",
    "   \r\n",
    "2. **Hidden Layers**:\r\n",
    "   - The network consists of multiple **Dense layers**, each with a specified number of neurons (e.g., 128, 64, 32). Each hidden layer uses the **ReLU activation function**, which introduces non-linearity and helps the model learn complex patterns.\r\n",
    "   - **L2 Regularization** is applied to the weights to prevent overfitting by penalizing large weights.\r\n",
    "   - **Dropout** is used in all but the final hidden layer, with a specified probability (`keep_prob`), to reduce overfitting by randomly deactivating neurons during training.\r\n",
    "\r\n",
    "3. **Output Layer**:\r\n",
    "   - A **Dense layer** with 10 neurons (for the 10 MNIST classes) and a **Softmax activation function** is used to output a probability distribution over the classes.\r\n",
    "\r\n",
    "#### **Model Flow**:\r\n",
    "- The input is passed through multiple hidden layers with ReLU activations, regularization, and dropout.\r\n",
    "- Finally, the output layer predicts the class probabilities, which are used to determine the most likely class (digit).\r\n",
    "  \r\n",
    "This architecture is typical for classification tasks, allowing the model to learn from data and generalize well on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "160bcc1b-05c3-4d24-998a-0ce6757094d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "        function that builds a neural network with the Keras library\n",
    "    \"\"\"\n",
    "    inputs = K.Input(shape=(nx,))\n",
    "\n",
    "    x = inputs\n",
    "    for i in range(len(layers)):\n",
    "        # add Dense layer\n",
    "        x = K.layers.Dense(layers[i],\n",
    "                           activation=activations[i],\n",
    "                           kernel_regularizer=K.regularizers.L2(lambtha))(x)\n",
    "\n",
    "        # apply Dropout except last layer\n",
    "        if i != len(layers) - 1 and keep_prob is not None:\n",
    "            x = K.layers.Dropout(1 - keep_prob)(x)\n",
    "\n",
    "    # create model\n",
    "    model = K.Model(inputs, x)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78e37c44-1103-4d77-947f-2da998268c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m100,480\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model parameters\n",
    "nx = x_train.shape[1]  # input dimension\n",
    "layers = [128, 64, 10]  # hidden layer and output sizes\n",
    "activations = ['relu', 'relu', 'softmax']  # activations for each hidden layer\n",
    "lambtha = 0.01  # L2 regularization strength\n",
    "keep_prob = 0.8  # dropout probability\n",
    "\n",
    "model = build_model(nx, layers, activations, lambtha, keep_prob)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af778dc-d8b5-49de-982a-99d8e1bf858e",
   "metadata": {},
   "source": [
    "### Model Optimization Overview\r\n",
    "\r\n",
    "The `optimize_model()` function configures the model with the **Adam optimizer** for efficient training:\r\n",
    "\r\n",
    "1. **Adam Optimizer**:\r\n",
    "   - **Alpha** (`learning_rate`): Controls the step size for weight updates.\r\n",
    "   - **Beta1** (`beta_1`): Momentum term, typically set to 0.9.\r\n",
    "   - **Beta2** (`beta_2`): Controls the moving average of squared gradients, typically set to 0.999.\r\n",
    "\r\n",
    "2. **Compilation**:\r\n",
    "   - **Loss**: **Categorical cross-entropy** is used for multi-class classification.\r\n",
    "   - **Metrics**: Tracks **accuracy** during training.\r\n",
    "\r\n",
    "This setup optimizes the model's weights, improving convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e219a6c-11ef-4806-80dc-ba279d9f1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "    Optimize\n",
    "\"\"\"\n",
    "\n",
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Adam_optimizer = K.optimizers.Adam(learning_rate=alpha,\n",
    "                                       beta_1=beta1,\n",
    "                                       beta_2=beta2)\n",
    "\n",
    "    network.compile(optimizer=Adam_optimizer,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fcfe9b6-f564-4b43-8563-96e520730dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "alpha = 0.001  # learning rate\n",
    "beta1 = 0.9  # Adam's beta1\n",
    "beta2 = 0.999  # Adam's beta2\n",
    "\n",
    "optimize_model(model, alpha, beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b94da-5d56-4c61-b1a6-fd77ee012e84",
   "metadata": {},
   "source": [
    "### Model Training Overview\r\n",
    "\r\n",
    "The `train_model()` function trains a neural network using **mini-batch gradient descent** with the following features:\r\n",
    "\r\n",
    "1. **Early Stopping**:\r\n",
    "   - Monitors validation loss to stop training if performance does not improve, avoiding overfitting.\r\n",
    "   - **Patience** defines how many epochs to wait before stopping.\r\n",
    "\r\n",
    "2. **Learning Rate Decay**:\r\n",
    "   - Reduces the learning rate over epochs using the inverse-time decayh}}\r\n",
    "     \\]\r\n",
    "   - Helps the model converge more smoothly as trainin\n",
    "  \n",
    "$$\n",
    "     \\text{lr} = \\frac{\\alpha}{1 + \\text{decay\\_rate} \\times \\text{epoch}}\n",
    "$$g progresses.\r\n",
    "\r\n",
    "3. **Save Best Model**:\r\n",
    "   - **ModelCheckpoint** saves the model with the best validation loss during training to avoid losing the best-performing model.\r\n",
    "\r\n",
    "4. **Training**:\r\n",
    "   - The model is trained with the provided data, labels, and training settings like batch size, epochs, and shuffle.\r\n",
    "\r\n",
    "This function ensures efficient training, with flexibility to adjust the learning rate, stop early if needed, and save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81b2c2-ffd6-4497-876e-ee5da5bce84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94c4ec46-b7af-4f82-8002-bb9a926515b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network, data, labels, batch_size,\n",
    "                epochs, validation_data=None, early_stopping=False,\n",
    "                patience=0, learning_rate_decay=False, alpha=0.1,\n",
    "                decay_rate=1, save_best=False, filepath=None,\n",
    "                verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "        Function that trains a model using mini-batch gradient descent\n",
    "    \"\"\"\n",
    "    callback = []\n",
    "    if early_stopping is True and validation_data is not None:\n",
    "        early_stop = K.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               patience=patience)\n",
    "\n",
    "        # add to callback list\n",
    "        callback.append(early_stop)\n",
    "\n",
    "    if learning_rate_decay and validation_data:\n",
    "        # function calculate new learning rate\n",
    "        def scheduler(epochs):\n",
    "            lr = alpha / (1 + decay_rate * epochs)\n",
    "            return lr\n",
    "\n",
    "        inv_time_decay = K.callbacks.LearningRateScheduler(\n",
    "            scheduler,\n",
    "            verbose=1)\n",
    "\n",
    "        # add to callback list\n",
    "        callback.append(inv_time_decay)\n",
    "\n",
    "    # save best model\n",
    "    if save_best:\n",
    "        save_best_model = K.callbacks.ModelCheckpoint(\n",
    "            filepath=filepath,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "\n",
    "        callback.append(save_best_model)\n",
    "\n",
    "    history = network.fit(x=data,\n",
    "                          y=labels,\n",
    "                          epochs=epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=validation_data,\n",
    "                          callbacks=[callback],\n",
    "                          verbose=verbose,\n",
    "                          shuffle=shuffle)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "423c8512-9db7-4ea1-936d-598a06da3ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7605 - loss: 1.7812 - val_accuracy: 0.8460 - val_loss: 1.1485\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.8428 - loss: 1.1678 - val_accuracy: 0.8573 - val_loss: 1.1014\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.8458 - loss: 1.1510 - val_accuracy: 0.8570 - val_loss: 1.0985\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.8463 - loss: 1.1438 - val_accuracy: 0.8626 - val_loss: 1.0853\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.8481 - loss: 1.1382 - val_accuracy: 0.8632 - val_loss: 1.0808\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "history = train_model(model, \n",
    "                      x_train, y_train_one_hot, \n",
    "                      batch_size, \n",
    "                      epochs, \n",
    "                      validation_data=(x_test, y_test_one_hot), \n",
    "                      early_stopping=True, patience=3, save_best=True, \n",
    "                      filepath='best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142209ce-0047-4634-a10d-c73ace50ef12",
   "metadata": {},
   "source": [
    "### Model Testing Overview\n",
    "\n",
    "The `test_model()` function evaluates the performance of a trained neural network on test data:\n",
    "\n",
    "- **Purpose**: It computes the loss and accuracy of the model on the given test data (`data`) and corresponding labels (`labels`).\n",
    "- **Parameters**:\n",
    "  - `network`: The trained Keras model to be tested.\n",
    "  - `data`: The input data used for testing.\n",
    "  - `labels`: The true labels corresponding to the test data.\n",
    "  - `verbose`: Controls the display of progress (default is `True`).\n",
    "  \n",
    "The function uses Keras' `evaluate()` method, which returns the loss and accuracy metrics based on the model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b430828b-443a-47a4-985c-1b1b654b4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "    Test neural network\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def test_model(network, data, labels, verbose=True):\n",
    "    \"\"\"\n",
    "        function that tests a neural network\n",
    "    \"\"\"\n",
    "    return network.evaluate(x=data,\n",
    "                            y=labels,\n",
    "                            verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4eb1883e-6bca-49b5-988a-9b7c4c3b7993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8388 - loss: 1.1441\n",
      "Test Loss: 1.0807803869247437\n",
      "Test Accuracy: 0.8632000088691711\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = test_model(model, x_test, y_test_one_hot)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae38c3-ae82-42bb-81e6-4a8f7e482aad",
   "metadata": {},
   "source": [
    "### Model Saving and Loading\n",
    "\n",
    "- **`save_model()`**: Saves the entire model (architecture, weights, optimizer state) to a file.\n",
    "  - **Parameters**: `network` (model), `filename` (file path).\n",
    "  \n",
    "- **`load_model()`**: Loads a previously saved model from a file.\n",
    "  - **Parameters**: `filename` (file path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d00ab83-1669-4504-b4ab-8dafa0906ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(network, filename):\n",
    "    \"\"\"\n",
    "        function that saves an entire model\n",
    "    \"\"\"\n",
    "    network.save(filename)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "        function that loads an entire model\n",
    "    \"\"\"\n",
    "    return K.models.load_model(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aae4660-b8df-41c7-8cef-1bf9c77dffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'mnist_model.keras')\n",
    "loaded_model = load_model('mnist_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2908379f-07c1-4dbd-97fa-6f714a5de502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8388 - loss: 1.1441\n",
      "Test Loss after loading model: 1.0807803869247437\n",
      "Test Accuracy after loading model: 0.8632000088691711\n"
     ]
    }
   ],
   "source": [
    "# Test the loaded model\n",
    "loss, accuracy = test_model(loaded_model, x_test, y_test_one_hot)\n",
    "print(f\"Test Loss after loading model: {loss}\")\n",
    "print(f\"Test Accuracy after loading model: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
