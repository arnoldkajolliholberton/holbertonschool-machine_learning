{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab28fbb",
   "metadata": {},
   "source": [
    "<img src=\"../holberton_logo.png\" alt=\"logo\" width=\"500\"/>\n",
    "\n",
    "# Regularization\n",
    "\n",
    "## What is regularization? What is its purpose?\n",
    "\n",
    "**Regularization is a technique used in machine learning to prevent overfitting**, which occurs when a model is too complex and captures noise in the training data instead of the underlying patterns. \n",
    "\n",
    "**The goal of regularization is to add a penalty term to the model's objective function that discourages overly complex models**, forcing the model to focus on the most important features of the data.\n",
    "\n",
    "<img src=\"figs/overfitting.png\" alt=\"logo\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figs/biasvariance.png\" alt=\"logo\" width=\"300\"/>\n",
    "\n",
    "\n",
    "\n",
    "In practice, regularization involves adding a term to the loss function that penalizes the model for large weights\n",
    "\n",
    "## What is are L1 and L2 regularization? What is the difference between the two methods?\n",
    "\n",
    "`L1` and `L2` regularization are techniques used in machine learning to prevent overfitting by adding a penalty term to the loss function. The penalty term is a function of the model weights, which encourages the weights to be small.\n",
    "\n",
    "- **`L1` regularization**, also known as Lasso regularization, **adds a penalty term that is proportional to the absolute value of the model weights**. This has the *effect of shrinking some of the weights to zero, effectively performing feature selection by removing some of the less important features*. L1 regularization can produce sparse models, where many of the weights are zero, making it useful for problems where only a small number of features are relevant.\n",
    "\n",
    "$$\n",
    "L1 = \\lambda \\cdot \\Sigma |w| \n",
    "$$\n",
    "\n",
    "- where\n",
    "    - $\\lambda$ is the regularization parameter\n",
    "    - $w$ represents the model's weights\n",
    "    - $\\Sigma |w|$ represents the sum of the absolute values of the weights.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **`L2` regularization**, also known as Ridge regularization, **adds a penalty term that is proportional to the square of the model weights**. This has the *effect of shrinking all of the weights towards zero, but rarely to zero*. L2 regularization can be useful for problems where all of the features are potentially relevant, but some may be more important than others. It can also be used to help prevent collinearity between features.\n",
    "\n",
    "$$\n",
    "L2 = \\lambda \\cdot \\Sigma (w)^2 \n",
    "$$\n",
    "\n",
    "- where\n",
    "    - $\\lambda$ is the regularization parameter\n",
    "    - $w$ represents the model's weights\n",
    "    - $\\Sigma w^2$ represents the square of the weights.\n",
    "\n",
    "\n",
    "\n",
    "## What is dropout?\n",
    "\n",
    "Dropout is a regularization technique used in neural networks to prevent overfitting. It works by randomly dropping out (i.e., setting to zero) a certain proportion of the neurons in a neural network during each training iteration. This means that each neuron is only active with a certain probability during a given training iteration, and the network must learn to perform well even when some of its neurons are missing.\n",
    "\n",
    "The effect of dropout is to prevent the network from relying too heavily on any one particular subset of neurons. This can help to prevent overfitting, as the network is forced to learn more robust representations that work well even when some neurons are missing.\n",
    "\n",
    "<img src=\"figs/dropout.png\" alt=\"logo\" width=\"500\"/>\n",
    "\n",
    "\n",
    "There are different variants of dropout, such as the standard dropout where neurons are dropped out with a fixed probability, and the spatial dropout where entire feature maps are dropped out. Dropout has been shown to be effective in improving the performance of neural networks, especially when the number of parameters is large and the amount of labeled data is limited.\n",
    "\n",
    "\n",
    "\n",
    "## What is early stopping?\n",
    "\n",
    "Early stopping is a regularization technique used in machine learning to prevent overfitting of a model. It involves monitoring the performance of a model on a validation set during the training process and stopping the training process when the performance on the validation set starts to deteriorate, i.e., the validation loss stops improving.\n",
    "\n",
    "The idea behind early stopping is that if the model is trained for too long, it may start to memorize the training data instead of learning the underlying patterns that generalize to new data. By stopping the training process early, we can prevent the model from overfitting and improve its ability to generalize to new data.\n",
    "\n",
    "<img src=\"figs/earlystop.png\" alt=\"logo\" width=\"300\"/>\n",
    "\n",
    "\n",
    "Early stopping is typically implemented by keeping track of the validation loss during training and stopping the training process when the validation loss stops improving for a certain number of epochs. The number of epochs to wait before stopping the training process is called the \"patience\" parameter, and it is a hyperparameter that needs to be tuned using cross-validation or other techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02da3e3",
   "metadata": {},
   "source": [
    "## Example: Regularization Techniques Using Keras \n",
    "\n",
    "\n",
    "### Define Model Architecture and Regularization Techniques\n",
    "\n",
    "We define a **Sequential model** using Keras, which allows us to stack layers sequentially. We add `Dense` layers to the model to create a feedforward neural network. These layers contain activation functions and regularization techniques like **Dropout, L1**, and **L2**.\n",
    "\n",
    "- **L1 Regularization**: Penalizes the absolute value of weights, promoting sparsity by pushing less important weights to zero, reducing model complexity and overfitting.\n",
    "\n",
    "- **L2 Regularization**: Penalizes the square of weights, constraining them to smaller values, preventing large weight updates and improving generalization by reducing overfitting.\n",
    "\n",
    "\n",
    "- **Dropout**: Randomly sets a fraction of neuron outputs to zero during training, preventing complex co-adaptations of neurons and encouraging robustness by effectively training multiple models within one.\n",
    "\n",
    "#### Code Example\n",
    "\n",
    "```python\n",
    "# Define a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l1(0.01)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "### Define Early Stopping and Train the Model with Early Stopping\n",
    "\n",
    "We define an **EarlyStopping callback to prevent overfitting**. \n",
    "\n",
    "*Early stopping monitors the validation loss during training and stops training when the loss stops decreasing, preventing the model from overfitting to the training data.*\n",
    "\n",
    "```python\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcc09ba",
   "metadata": {},
   "source": [
    "\n",
    "## What is data augmentation?\n",
    "\n",
    "Data augmentation is a technique used in machine learning to increase the size and diversity of a dataset by generating new examples from the existing data. It involves applying a set of transformations or modifications to the original data to create new data points. These transformations can be simple, such as flipping or rotating an image, or more complex, such as adding noise or changing the color balance.\n",
    "\n",
    "Data augmentation is particularly useful when the size of the training dataset is limited, as it can help to prevent overfitting and improve the generalization ability of the model. By generating new data points, data augmentation can also help to address class imbalance problems and improve the performance of the model on rare or underrepresented classes.\n",
    "\n",
    "<img src=\"figs/dataaug.png\" alt=\"logo\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "## What are the pros and cons of the above regularization methods?\n",
    "\n",
    "- **L1 regularization**:\n",
    "    - Pros:\n",
    "        - Can lead to sparse feature selection, which can improve model interpretability and reduce overfitting by removing irrelevant features.\n",
    "        - Works well when there are only a few important features.\n",
    "    - Cons:\n",
    "        - Can be sensitive to correlated features.\n",
    "        - Computationally expensive to optimize.\n",
    "        \n",
    "        \n",
    "- **L2 regularization**:\n",
    "    - Pros:\n",
    "        - Encourages smaller weights, which can help prevent overfitting.\n",
    "        - Computationally efficient to optimize.\n",
    "    - Cons:\n",
    "        - Does not lead to sparse feature selection.\n",
    "        - May not perform well when there are only a few important features.\n",
    "    \n",
    "    \n",
    "- **Dropout**:\n",
    "    - Pros:\n",
    "        - Simple to implement and computationally efficient.\n",
    "        - Can help prevent overfitting by reducing the reliance on individual neurons in the network.\n",
    "        - Can improve model robustness and generalization.\n",
    "    - Cons:\n",
    "        - Can increase the training time required for convergence.\n",
    "        - Can reduce the representational capacity of the network if the dropout rate is too high.\n",
    "        \n",
    "        \n",
    "- **Early stopping**:\n",
    "    - Pros:\n",
    "        - Simple to implement and computationally efficient.\n",
    "        - Can prevent overfitting and improve model generalization by stopping the training process before the model starts to overfit the training data.\n",
    "        - Can lead to faster training times.\n",
    "    - Cons:\n",
    "        - Requires the use of a validation set to determine when to stop training.\n",
    "        - May not always result in the best performance, as the optimal stopping point can depend on the specific dataset and model architecture.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ed5ef",
   "metadata": {},
   "source": [
    "### Happy coding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
