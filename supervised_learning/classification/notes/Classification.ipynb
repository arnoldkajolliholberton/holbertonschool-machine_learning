{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b38a9e8-dc14-4397-8515-5667efdc3ab3",
   "metadata": {},
   "source": [
    "### Binary Classification (Images of 0 and 1)  \n",
    "Binary classification is a type of supervised learning where the goal is to assign an input to one of two categories. In the case of images of 0 and 1, a model learns to distinguish between these two digits. Given an input image, the model outputs a probability score, which is then thresholded to classify the image as either a 0 or a 1.\n",
    "\n",
    "### Fundamentals of a Single Neuron  \n",
    "A single neuron in a neural network is a fundamental unit that processes inputs and produces an output using three key components:  \n",
    "\n",
    "- **Weights**: Each input is multiplied by a weight, which determines the importance of that input. The model learns optimal weight values during training.  \n",
    "- **Bias**: A bias term is added to shift the output, allowing the neuron to better fit the data even when inputs are zero.  \n",
    "- **Activation Function**: The weighted sum of inputs plus bias is passed through an activation function, such as the sigmoid function, to introduce non-linearity and output a probability (0 to 1) in the case of binary classification.\n",
    "\n",
    "![Neuron](https://bpb-us-e1.wpmucdn.com/blogs.cornell.edu/dist/a/1688/files/2015/09/VqOpE-1c4xc4y.jpg)\r",
    "s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20fb75-e869-4aae-97f0-71487315bb01",
   "metadata": {},
   "source": [
    "### **Forward Propagation: Role, Intuition, and Formula**  \n",
    "\n",
    "#### **Role**  \n",
    "Forward propagation computes the neuron's output by applying learned weights, bias, and an activation function to the input data.\n",
    "\n",
    "#### **Intuition**  \n",
    "1. **Linear Combination**: Inputs are weighted and summed with a bias.  \n",
    "2. **Non-Linearity**: The sigmoid function maps this sum to a probability between 0 and 1.  \n",
    "3. **Prediction**: The output represents the probability of class 1.\n",
    "\n",
    "#### **Formula**  \n",
    "Linear transformation\n",
    "\n",
    "$$\n",
    "   Z = W X + b\n",
    "$$\n",
    "\n",
    "\n",
    "Activation (Sigmoid)  \n",
    "$$\n",
    "   A = \\frac{1}{1 + e^{-Z}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a800dd-f223-448e-b55c-9bfa499d98e0",
   "metadata": {},
   "source": [
    "### **Cost: Role, Intuition, and Formula**  \n",
    "\n",
    "#### **Role**  \n",
    "The cost function measures how well the neuron’s predictions match the true labels. It quantifies the error and guides weight updates during training.\n",
    "\n",
    "#### **Intuition**  \n",
    "- If predictions \\( A \\) are close to true labels \\( Y \\), the cost is low.  \n",
    "- If predictions are incorrect, the cost increases.  \n",
    "- The **log loss (binary cross-entropy)** is used for binary classification, penalizing incorrect confident predictions more heavily.\n",
    "\n",
    "#### **Formula (Log Loss for Logistic Regression)**  \n",
    "\n",
    "$$\n",
    "J = -\\frac{1}{m} \\sum \\left[ Y \\log(A) + (1 - Y) \\log(1 - A) \\right]\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ Y $ → True labels, shape $ (1, m) $.  \n",
    "- $ A $ → Predicted probabilities, shape $ (1, m) $. \n",
    "- $ m $ → Number of samples  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc7d66-02ad-4b7d-b503-28d728a72b5e",
   "metadata": {},
   "source": [
    "### **Evaluation: Role, Intuition, and Process**  \n",
    "\n",
    "#### **Role**  \n",
    "Evaluation checks how well the neuron classifies inputs by generating predictions and computing the cost.\n",
    "\n",
    "#### **Intuition**  \n",
    "1. **Forward Propagation**: The neuron processes input \\( X \\) to compute predicted probabilities \\( A \\).  \n",
    "2. **Cost Calculation**: Measures how far predictions \\( A \\) deviate from true labels \\( Y \\) using log loss.  \n",
    "3. **Thresholding for Classification**: Since \\( A \\) represents probabilities, values \\( \\geq 0.5 \\) are classified as 1, and values \\( < 0.5 \\) as 0. This converts continuous outputs into discrete class labels.  \n",
    "\n",
    "#### **Process in Code**  \n",
    "1. **Run forward propagation** → `A = self.forward_prop(X)`  \n",
    "2. **Compute cost** → `cost = self.cost(Y, A)`  \n",
    "3. **Make predictions** → `result = np.where(A >= 0.5, 1, 0)`  \n",
    "4. **Return predictions and cost**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf840ac-54a7-42a2-9c98-7cab44333322",
   "metadata": {},
   "source": [
    "### **Backpropagation: Role, Intuition, and Process**  \n",
    "\n",
    "#### **Role**  \n",
    "Backpropagation is the key algorithm for training the neuron by adjusting weights and bias to minimize prediction error. It calculates gradients (partial derivatives) of the cost function with respect to the parameters and updates them using **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Intuition**  \n",
    "1. **Error Signal**: The neuron computes the difference between predicted outputs \\( A \\) and true labels \\( Y \\) (i.e., \\( A - Y \\)).  \n",
    "2. **Gradient Calculation**:  \n",
    "   - **Weight Gradient**: Measures how much each weight contributes to the error. Computed using the derivative of the cost function with respect to weights.  \n",
    "   - **Bias Gradient**: Measures the impact of the bias term on the error.  \n",
    "3. **Parameter Update (Gradient Descent)**:  \n",
    "   - Weights and bias are adjusted by moving them in the opposite direction of the gradients, scaled by the learning rate \\( \\alpha \\).  \n",
    "   - This reduces the cost function, improving predictions over time.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Mathematical Formulation**  \n",
    "Given:  \n",
    "- \\( m \\) → Number of samples  \n",
    "- \\( W \\) → Weight vector (shape \\( (1, nx) \\))  \n",
    "- \\( b \\) → Bias (scalar)  \n",
    "- \\( X \\) → Input matrix (shape \\( (nx, m) \\))  \n",
    "- \\( A \\) → Predicted output (shape \\( (1, m) \\))  \n",
    "- \\( Y \\) → True labels (shape \\( (1, m) \\))  \n",
    "- \\( \\alpha \\) → Learning rate  \n",
    "\n",
    "1. **Compute Gradients:**  \n",
    "   - **Weight Gradient**:  \n",
    "$$\n",
    "     \\frac{\\partial J}{\\partial W} = \\frac{1}{m} (A - Y) X^T\n",
    "$$\n",
    "   - **Bias Gradient**:  \n",
    "$$\n",
    "     \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum (A - Y)\n",
    "$$\n",
    "\n",
    "3. **Update Parameters:**  \n",
    "   - **Weight Update**:  \n",
    "$$\n",
    "     W = W - \\alpha \\frac{\\partial J}{\\partial W}\n",
    "$$\n",
    "   - **Bias Update**:  \n",
    "$$\n",
    "     b = b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Process in Code**  \n",
    "1. **Compute Gradients:**  \n",
    "   - `grad_w = 1 / m * np.matmul((A - Y), X.T)`  \n",
    "   - `grad_b = 1 / m * np.sum(A - Y)`  \n",
    "2. **Update Weights and Bias:**  \n",
    "   - `self.__W -= alpha * grad_w`  \n",
    "   - `self.__b -= alpha * grad_b`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f008d-b4e4-480f-aff9-fefbdae8d2fa",
   "metadata": {},
   "source": [
    "### **Training the Neuron: High-Level Overview**  \n",
    "\n",
    "#### **Role**  \n",
    "The `train` method allows the neuron to learn from the training data by iterating over multiple cycles, improving its ability to make accurate predictions.\n",
    "\n",
    "#### **Intuition**  \n",
    "1. **Input Validation**: Ensures valid `iterations` and `alpha` values are provided.\n",
    "2. **Iterative Learning**: Over multiple iterations:\n",
    "   - **Forward Propagation**: Computes predictions.\n",
    "   - **Backpropagation (Gradient Descent)**: Updates weights and bias to reduce error.\n",
    "   - **Evaluation**: Assesses the current performance.\n",
    "3. **Return**: After the specified number of iterations, return the final predictions and cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cffb5c-df8a-4917-afc8-bbd723ceb8b9",
   "metadata": {},
   "source": [
    "### **Key Idea of Gradient Descent in the Code**\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function by updating the parameters (weights and biases) iteratively. The idea is to compute the gradient (partial derivatives) of the cost function with respect to each parameter and update them in the direction that reduces the cost.\n",
    "\n",
    "In this **NeuralNetwork** class, gradient descent is implemented in the `gradient_descent` method. The goal is to adjust the weights (`W1`, `W2`) and biases (`b1`, `b2`) to improve the model's predictions over time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Breaking Down the Multiplications**\n",
    "The key mathematical operations in gradient descent involve partial derivatives of the cost function with respect to the parameters. Here’s the intuition behind each multiplication:\n",
    "\n",
    "1. **Error at Output Layer (dz2)**\n",
    "$$\n",
    "   dz2 = A2 - Y\n",
    "$$\n",
    "   - `A2` is the predicted output.\n",
    "   - `Y` is the actual label.\n",
    "   - The subtraction calculates the difference between predicted and actual values, which is the error signal that needs to be backpropagated.\n",
    "\n",
    "2. **Gradient of Weights in the Output Layer (d__W2)**\n",
    "$$\n",
    "   dW2 = \\frac{1}{m} \\cdot (dz2 \\cdot A1^T)\n",
    "$$\n",
    "   - `dz2` (error at the output layer) is multiplied by the activations of the hidden layer (`A1`).\n",
    "   - This tells us how much each weight in `W2` contributed to the final error.\n",
    "   - The division by `m` averages the gradient over all training examples.\n",
    "\n",
    "3. **Gradient of Bias in the Output Layer (d__b2)**\n",
    "$$\n",
    "   db2 = \\frac{1}{m} \\sum dz2\n",
    "$$\n",
    "   - Since bias terms are independent of input values, we sum over all examples and average.\n",
    "\n",
    "4. **Error at the Hidden Layer (dz1)**\n",
    "$$\n",
    "   dz1 = (W2^T \\cdot dz2) \\cdot A1(1 - A1)\n",
    "$$\n",
    "   - Backpropagating the error from the output layer to the hidden layer:\n",
    "     - `W2^T * dz2` propagates the error backwards.\n",
    "     - `A1(1 - A1)` accounts for the derivative of the sigmoid activation function (chain rule in calculus).\n",
    "\n",
    "5. **Gradient of Weights in the Hidden Layer (d__W1)**\n",
    "$$\n",
    "   dW1 = \\frac{1}{m} \\cdot (dz1 \\cdot X^T)\n",
    "$$\n",
    "   - `dz1` is multiplied by the input values `X` to determine how much `W1` should change.\n",
    "\n",
    "6. **Gradient of Bias in the Hidden Layer (d__b1)**\n",
    "$$\n",
    "   db1 = \\frac{1}{m} \\sum dz1\n",
    "$$\n",
    "   - Bias gradients are computed similarly by summing over all examples.\n",
    "\n",
    "7. **Updating Weights and Biases**\n",
    "$$\n",
    "   W2 = W2 - \\alpha dW2, \\quad b2 = b2 - \\alpha db2\n",
    "$$\n",
    "$$\n",
    "   W1 = W1 - \\alpha dW1, \\quad b1 = b1 - \\alpha db1\n",
    "$$\n",
    "   - The parameters are updated by moving them **in the opposite direction** of the gradient (scaled by the learning rate `alpha`).\n",
    "   - This ensures the model minimizes the error step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition for Why These Multiplications Are Needed**\n",
    "- The weight updates use **matrix multiplication** because each weight contributes to multiple outputs.\n",
    "- The gradient of the activation function **scales the error** to properly adjust the influence of each neuron.\n",
    "- **Backpropagation** efficiently distributes the error from the output layer to the hidden layer using derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906f0b5-029c-4fa1-93a5-501dad9d8efd",
   "metadata": {},
   "source": [
    "The use of **transposed matrices** in gradient descent for neural networks is crucial for ensuring the correct dimensions of the matrix operations during the backpropagation step. Let’s break down why transposing the matrices is needed:\n",
    "\n",
    "### **1. Matrix Multiplication Dimensions**\n",
    "When performing backpropagation, we need to compute gradients for the weights, and these gradients involve matrix multiplication. Matrix multiplication follows specific rules about the dimensions of the matrices being multiplied. \n",
    "\n",
    "#### **For the weight gradient in the output layer:**\n",
    "The gradient for the weights in the output layer is computed as:\n",
    "$$\n",
    "dW2 = \\frac{1}{m} \\cdot (dz2 \\cdot A1^T)\n",
    "$$\n",
    "Where:\n",
    "- `dz2` is the error (a vector of size `1 x m` where `m` is the number of training examples).\n",
    "- `A1` is the activation from the hidden layer (a vector of size `nodes x m`, where `nodes` is the number of neurons in the hidden layer).\n",
    "\n",
    "Here’s the reasoning for the **transpose of A1**:\n",
    "\n",
    "- `A1` is the activations from the hidden layer (size `nodes x m`).\n",
    "- The multiplication needs to compute how each weight in `W2` contributes to the errors in `dz2`. Since `W2` is of size `1 x nodes`, you need to multiply it with `A1` (which has size `nodes x m`).\n",
    "- The multiplication `dz2 * A1^T` ensures that we align the error `dz2` with the activations `A1`. **By transposing `A1`**, you get a matrix of size `m x nodes`, which ensures the multiplication with `dz2` (size `1 x m`) is valid and results in the gradient for `W2` of size `1 x nodes`.\n",
    "\n",
    "Without the transpose of `A1`, the matrix multiplication would not be possible because the dimensions wouldn’t align. \n",
    "\n",
    "#### **For the weight gradient in the hidden layer:**\n",
    "The gradient for the weights in the hidden layer is computed as:\n",
    "$$\n",
    "dW1 = \\frac{1}{m} \\cdot (dz1 \\cdot X^T)\n",
    "$$\n",
    "Where:\n",
    "- `dz1` is the error propagated back to the hidden layer (a vector of size `nodes x m`).\n",
    "- `X` is the input data (a matrix of size `nx x m`, where `nx` is the number of input features).\n",
    "\n",
    "The reasoning behind **transposing `X`** is similar to the previous case:\n",
    "\n",
    "- `dz1` is of size `nodes x m` because there are `nodes` neurons in the hidden layer, and `m` is the number of training examples.\n",
    "- The weights `W1` are of size `nodes x nx` (where `nx` is the number of input features).\n",
    "- To compute the gradient of `W1`, you need to multiply `dz1` (which has size `nodes x m`) with `X^T` (the transpose of the input data, which has size `m x nx`).\n",
    "- This multiplication produces a matrix of size `nodes x nx`, which is the correct shape for the gradient of `W1`.\n",
    "\n",
    "Again, without transposing `X`, the matrix multiplication wouldn’t work because the dimensions wouldn’t align.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d63e1a-a81e-4b47-8a86-50c84e58d289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
