{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3c29ec-f2fa-45dd-b919-235cd3f89704",
   "metadata": {},
   "source": [
    "# Fashion MNIST Classification using Machine Learning\n",
    "\n",
    "The Fashion MNIST dataset is a popular benchmark dataset in machine learning that contains grayscale images of fashion items such as shirts, shoes, bags, and dresses. \n",
    "\r\n",
    "\r\n",
    "### Problem Description:\r\n",
    "The goal of this project is to classify the images of fashion items into one of 10 categories. These categories include:\r\n",
    "1. T-shirt/top\r\n",
    "2. Trouser\r\n",
    "3. Pullover\r\n",
    "4. Dress\r\n",
    "5. Coat\r\n",
    "6. Sandal\r\n",
    "7. Shirt\r\n",
    "8. Sneaker\r\n",
    "9. Bag\r\n",
    "10. Ankle boot\r\n",
    "\r\n",
    "Each image in the dataset is a 28x28 pixel grayscale image, which is flattened into a 1D array of 784 pixels. The task is to predict the correct class label (from the 10 categories) for a given image based on ihproach works best for th\n",
    "\n",
    "![Fashion MNIST Dataset](https://datasets.activeloop.ai/wp-content/uploads/2022/09/Fashion-MNIST-dataset-Activeloop-Platform-visualization-image.webp)\n",
    "is type of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6ae9e-fccd-4362-a7e4-6790ae30dbbc",
   "metadata": {},
   "source": [
    "## Key Steps of our Machine Learning problem\n",
    "\n",
    "\n",
    "![Machine Learning Pipeline](https://valohai.com/assets/img/manual-pipeline.png)\n",
    "\n",
    "## 1. Problem Definition\r\n",
    "The objective of this project is to classify fashion-related items into 10 distinct categories using the Fashion MNIST dataset. This i ** a multi-class classification pro*le**m where the task is to assign each image to one 10 categories, part of the datasett\r\n",
    "\r\n",
    "## 2. Data Collection\r\n",
    "The Fashion MNIST dataset contains:\r\n",
    "- 60,000 training images\r\n",
    "- 10,000 testing images\r\n",
    "- Each image is a 28x28 grayscale image, flattened into a vector# of 784 pixels.\r\n",
    "\r\n",
    "## 3. Data Preprocessing\r\n",
    "- **Loading Data:** Use libraries like TensorFlow or scikit-learn to load the dataset.\r\n",
    "- **Data Normalization:** Scale pixel values to the range [0, 1] to improve model performance.\r\n",
    "- **Train-Test Split:** Ensure proper splitting of the dataset into training and testing sets.\r\n",
    "- **Label Encoding:** Encode the categorical lric #values, if necessary.\r\n",
    "\r\n",
    "## 4. Exploratory Data Analysis (EDA)\r\n",
    "- **Visualize Samples:** Display random images from the dataset to understand the distribution of classes.\r\n",
    "- **Class Distribution:** Check the balance of class labels to identify any issues related to data imbalance.\r\n",
    "- **Image Dimensions:** Verify the shape of imag#es and their pixel values.\r\n",
    "\r\n",
    "## 5. Model Selection\r\n",
    "Choose one or more classical machine learning algorithms ttrain the model:\r\n",
    "-*Lostic Regression**\r\n",
    "- **K-aVeor achines (SV**\r",
    " **Dec#ision ees**\r\n",
    "- **Random Forest**\r\n",
    "\r\n",
    "## 6. Model Training\r\n",
    "- **Feature Extraction:** Flatten the 28x28 pixel images into 784-dimensional vectors.\r\n",
    "- **Model Training:** Train the see #the model's performance during training.\r\n",
    "\r\n",
    "## 7. Model Evaluation\r\n",
    "- **Accuracy:** Evaluate the performance using accuracy metrics on the test dataset.\r\n",
    "- **Confusion Matrix:** Analyze misclassifications with a confusion matrix to identify where the model is failing.\r\n",
    "- **Other Metrics:** Optionally, calculate precin #us8ng random search to improve model performance.\r\n",
    "\r\n",
    "## 9. Model Comparison\r\n",
    "- Compare the performance of various classical models (e.g., Logistic Regression vs. KNN) based on accuracy, training time, and complexity.\r\n",
    "- **Select Best Model:** Choose themod#e9.  a web service or an API for real-time predictions.\r\n",
    "\r\n",
    "## 11. Conclusion and Future Work\r\n",
    "- **Summarize Results:** Discuss the final model's performance and its suitability for the task.\r\n",
    "- **Future Work:** Mention possible improvements, such as using deep learnng models (e.g., CNNs) for better accuracy or adding more data.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be060e66-f000-4845-9523-c675a3e39813",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371a80e-89a8-4a6f-a156-760e1475ff6b",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "In this step, we will load the Fashion MNIST dataset, which consists of `60,000` training images and `10,000` test images. Each image is a `28x28` grayscale image, and the dataset is divided into input features (pixel values) and labels (corresponding fashion item categories). We will use libraries like `tensorflow` and `keras` to easily load the dataset into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5dfed8-ff84-4217-bf61-db63f03efc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "692b207c-3208-4ad1-b62f-cfb35067b47e",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis \n",
    "\n",
    "In this step, we visually explore the dataset to understand its structure and gain insights into the distribution of the data. EDA helps us assess potential issues such as class imbalance, missing data, or variations within the dataset. The following steps involve displaying sample images, visualizing the dataset using Matplotlib, and checking the distribution of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b838f2a-76b3-4d69-9f42-843e001eaa67",
   "metadata": {},
   "source": [
    "#### Displaying Sample Images as a List of Values\n",
    "\n",
    "We can display a few images and their corresponding pixel values to get an understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168cefb-853e-47fd-8911-8eb29e49d6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ef2088-f0cc-44d2-82fb-d14d9d71ee24",
   "metadata": {},
   "source": [
    "#### Displaying Images with Matplotlib\n",
    "We can use Matplotlib to visualize some sample images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a934c-745d-47ad-8465-29f357c21e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58129cd5-3d63-489e-86cf-b364c5ad7bbf",
   "metadata": {},
   "source": [
    "#### Class Distribution\n",
    "Next, we visualize the class distribution to check for any imbalances in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76388e54-dccc-4f9f-b8fe-e808866ea654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "858cf2d3-f980-4f70-8b36-f3c0efb7cc82",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "In this step, we prepare the dataset for model training. This includes scaling the pixel values to the range `[0, 1]`, splitting the data into training and testing sets , and encoding the labels if necessary. These preprocessing steps help improve model performance by ensuring the data is in an optimal format for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c48aaa-e5a3-4a20-a18f-9d441698283a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3bf83-0306-41d0-863c-c67afd149a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "303fa6ad-ddf4-4ad5-971a-733d47f47687",
   "metadata": {},
   "source": [
    "## Model Selection and Intuition\n",
    "In this step, we choose one or more classical machine learning algorithms to train our model on the Fashion MNIST dataset. Below, Iâ€™ll explain each of the selected algorithms, how they work, and their intuition behind solving the multi-class classification problem.\n",
    "\n",
    "### 1. Logistic Regression\n",
    "Logistic Regression is a linear model used for binary and multi-class classification. It works by fitting a linear decision boundary to the data and using the sigmoid (or softmax for multi-class) function to predict the probabilities of each class.\n",
    "\n",
    "- The model will learn a linear relationship between the pixel values (features) and the class labels (fashion item categories).\n",
    "- Logistic Regression is relatively simple and works well when classes are linearly separable.\n",
    "- In the context of Fashion MNIST, it will attempt to fit a linear boundary that separates images of different fashion items, which may not always be the most optimal given the complex nature of image data. But it can still provide a solid baseline.\n",
    "\n",
    "![Logistic Regression](https://miro.medium.com/v2/resize:fit:1400/0*AT1W9oWnZUKp15ym.jpeg)\n",
    "\n",
    "### 2. Decision Trees\n",
    "\n",
    "A Decision Tree is a non-linear, hierarchical model that makes predictions by splitting the dataset into branches based on feature values. It recursively divides the data into subsets until a stopping criterion is met, forming a tree-like structure. Each node represents a decision based on feature values, and each leaf represents a predicted class.\n",
    "\n",
    "- Decision Trees will create splits based on pixel values to divide the images into different classes.\n",
    "- For example, it may learn to distinguish items like \"sneakers\" from \"ankle boots\" by looking at specific pixel patterns in the images that differ between those two categories.\n",
    "- Decision Trees can model non-linear relationships and handle complex decision boundaries between different classes. However, they may overfit the training data if not properly tuned. They are easy to interpret, which is a benefit when understanding how decisions are made for classifications.\n",
    "\n",
    "![Decision Trees](https://www.researchgate.net/publication/336387004/figure/fig7/AS:962205759590416@1606419138485/Visualization-of-an-oblique-decision-tree-learned-on-FashionMNIST-Xiao-etal-2017-with.png)\n",
    "\n",
    "\n",
    "### 3. Random Forest\n",
    "Random Forest is an ensemble method that combines multiple Decision Trees to make more robust and accurate predictions. It works by creating a set of decision trees with random subsets of the data and features. The final prediction is made by averaging the predictions of all trees in the forest (for regression) or by majority voting (for classification).\n",
    "\n",
    "- Random Forest will combine multiple Decision Trees to improve the overall classification performance.\n",
    "- It will look for diverse decision rules by training different trees on random subsets of the data, which helps it generalize better than a single decision tree.\n",
    "- Random Forest can handle a high-dimensional feature space like Fashion MNIST efficiently and capture complex patterns by combining the strengths of multiple trees\n",
    "\n",
    "![Random Trees](https://miro.medium.com/v2/resize:fit:592/1*i0o8mjFfCn-uD79-F1Cqkw.png)\n",
    "\n",
    "\n",
    "\n",
    "### 4. K-Nearest Neighbors (KNN)\n",
    "KNN is a non-parametric, instance-based learning algorithm that classifies data points based on the majority class of their k-nearest neighbors in the feature space. It doesnâ€™t build a model but instead stores the training dataset and makes predictions based on the distance to the closest data points during testing.\n",
    "- KNN will classify each test image by finding the most similar images in the training dataset and assigning the majority class among them.\n",
    "- It works well for datasets where similar instances tend to have similar labels. Since Fashion MNIST contains recognizable patterns (e.g., images of shirts, dresses, and sneakers), KNN can leverage these patterns for classification.\n",
    "\n",
    "![Random Forest](https://miro.medium.com/v2/resize:fit:1010/1*wj0YGKlKDu0nR50xhaBriw.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd18649-da45-46ff-8d8f-d840ec877147",
   "metadata": {},
   "source": [
    "### Train Logistic Regression Model\n",
    "\n",
    "We train a **Logistic Regression** model on the **Fashion MNIST** dataset using **Scikit-Learn**. The images, originally `28Ã—28` pixels, are flattened into `1D` vectors of `784` features before training. The model is configured with **1000 iterations** to ensure convergence. After fitting on the training data, it evaluates performance on the test set, printing the **classification accuracy** as a percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3220e-31ae-42ca-ada7-c203caebdf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac75f028-37ea-4a9d-8dfb-3bc53d9fe1b1",
   "metadata": {},
   "source": [
    "### Decision Trees Model\n",
    "\n",
    "Next, we train a **Decision Tree Classifier** on the **Fashion MNIST** dataset using **Scikit-Learn**. Again, the `28Ã—28` pixel images are flattened into `1D` vectors of `784` features before training. The model learns a tree-based decision structure to classify images based on pixel intensity patterns. After fitting on the training data, we evaluate performance on the test set, printing the **classification accuracy** as a percentage. Decision trees can capture complex patterns but may overfit without proper tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47009f-1a93-41fb-9fff-8e4765ad85d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8e693ec-aa75-4ed7-b757-48ac65d8354f",
   "metadata": {},
   "source": [
    "### Random Forest Model\n",
    "\n",
    "Our third model is a **Random Forest Classifier**. Similarly, the images are flattened into `1D` vectors of `784` features before training. The model consists of **100 decision trees** (**n_estimators=100**) that work together to classify images by aggregating their predictions through majority voting. After training, it evaluates performance on the test set and prints the **classification accuracy** as a percentage.  \r\n",
    "\r\n",
    "### How Random Forest Works in Fashion MNIST  \r\n",
    "Random Forest builds multiple **decision trees**, each trained on a random subset of the data and features. This **ensemble learning** approach helps reduce overfitting, making the model more robust than a single decision tree. Since Fashion MNIST contains pixel-based patterns, Random Forest can capture nonlinear relationships and improve classification accuracy.  \r\n",
    "\r\n",
    "### Difference Between Random Forest and Decision Trees  \r\n",
    "- **Decision Trees**: A single tree that recursively splits data based on feature conditions, often leading to **overfitting** if not pruned properly.  \r\n",
    "- **Random Forest**: An **ensemble** of decision trees that generalizes better by averaging multiple predictions, reducing variance and overfitting. This typically results in **higher accuracy** than an individual decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b998d6-e069-4830-8d67-6337cbe7c3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5df4ab7-d355-41be-bc5a-13d287a10ce8",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "### **K-Nearest Neighbors (KNN) Model**  \r\n",
    "\r\n",
    "Our final model is a **K-Nearest Neighbors (KNN) Classifier**. As in the previous cases, the images are flattened into **1D vectors of 784 features** before training. The model classifies each test image by finding the **5 nearest neighbors** (**n_neighbors=5**) in the training set and assigning the most common label among them. After training, it evaluates performance on the test set and prints the **classification accuracy** as a percentage.  \r\n",
    "\r\n",
    "### **How K-Nearest Neighbors Works in Fashion MNIST**  \r\n",
    "KNN is a **non-parametric, instance-based** algorithm that does not learn an explicit model but instead **stores** the training data. During classification, it computes the distance (e.g., **Euclidean distance**) between the test image and all training images, selecting the **5 closest** samples to determine the predicted label by majority vote. Since **Fashion MNIST has 10 classes**, using **k=5** ensures that predictions are more stable compared to **k=3**, reducing the effect of noise while maintaining flexibility. This method works well when pixel values form **distinct clusters**, but it can be computationally expensive for large datasets.  \r\n",
    "\r\n",
    "### **Difference Between KNN and Decision Trees**  \r\n",
    "- **KNN**: A **lazy learner** that does not build a model during training but instead makes predictions by comparing each new sample to the stored dataset. It is **computationally expensive** at prediction time, especially for large datasets.  \r\n",
    "- **Decision Trees**: A **greedy learner** that builds a hierarchical tree during training, making predictions faster but prone to **overfitting** if not pruned properly.runed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47be20-45cd-465b-b5c9-ccde607f5e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45e95125-fb01-4bc3-8c96-8db44e7e701d",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "The Decision Tree model performed the worst (79.07%) because it tends to overfit on high-dimensional data and struggles with generalization. Logistic Regression performed better (84.37%) as it models linear decision boundaries effectively. K-Nearest Neighbors (KNN) improved further (85.41%) since it captures local patterns well, though it can be sensitive to noise. Random Forest achieved the highest accuracy (87.73%) by leveraging multiple decision trees and averaging their predictions, reducing variance and improving generalization.\n",
    "\n",
    "These results highlight that ensemble methods like Random Forest tend to perform best in classical machine learning tasks, while simple models like Decision Trees can struggle on complex datasets like Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95852b-7708-411a-b440-71f390ba193d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "546b8f79-7f35-4d44-94e6-9c0d58e9c998",
   "metadata": {},
   "source": [
    "### Random Forest Confusion Matrix Analysis\n",
    "\n",
    "The confusion matrix reveals that the Random Forest model performs well on distinct classes like Sneakers and Bags, where misclassification is minimal. However, it struggles with visually similar categories such as T-shirts vs. Shirts and Pullover vs. Coat, leading to more misclassifications. This aligns with the modelâ€™s reliance on decision trees, which can struggle with subtle pixel variations. Overall, while Random Forest achieves the highest accuracy (87.73%), its performance varies by class, highlighting areas for potential improvement, such as feature engineering or using more sophisticated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fe3ae-e15a-4cfd-9d0c-ece91fff8654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4867c8d5-d081-40aa-898a-3ea2e4247621",
   "metadata": {},
   "source": [
    "### Simulating Real-World Testing with Your Trained Model\n",
    "\n",
    "To evaluate the model in a real-world scenario, we tested it on an external image by preprocessing it to match the Fashion MNIST dataset (grayscale, resized to 28Ã—28, and normalized). The trained Random Forest model then predicted the most likely clothing category. This simulation demonstrates how the model generalizes beyond the training set, providing insight into its practical application for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c657c9c-f6d3-425e-ae1c-b2a19cbfd6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c5331ee-3b74-4a37-b4ae-b05a845f0d26",
   "metadata": {},
   "source": [
    "### Summary: Fashion MNIST Classification with Classical Machine Learning\r\n",
    "\r\n",
    "This project explored **Fashion MNIST** classification using classical machine learning algorithms. We followed a structured pipeline:  \r\n",
    "\r\n",
    "1. **Data Collection & Preprocessing** â€“ Loaded the dataset, normalized pixel values, and prepared labels for model training.  \r\n",
    "2. **Exploratory Data Analysis** â€“ Visualized sample images, pixel distributions, and class balance to understand dataset characteristics.  \r\n",
    "3. **Model Selection & Training** â€“ Implemented **Logistic Regression, Decision Trees, Random Forest, and K-Nearest Neighbors (KNN)** to classify fashion items, evaluating their performance.  \r\n",
    "4. **Evaluation & Interpretation** â€“ Measured accuracy across models, analyzed class-wise performance using a **confusion matrix**, and identified Random Forest as the best-performing model.  \r\n",
    "5. **Real-World Testing** â€“ Used a trained **Random Forest model** to predict an unseen image, simulating practical applicatThisrall, this project demonstrated how **classical algorithms** can effectively classify images without deep learning, achieving competitive accuracy while providing interpretability and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
